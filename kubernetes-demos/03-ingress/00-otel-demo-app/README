# Lab 00: Deploy OpenTelemetry Demo Application

## Lab Overview

This lab deploys the OpenTelemetry (OTel) Demo application on AWS EKS as the foundation for learning Kubernetes Ingress controllers. The OTel Demo is a microservices-based e-commerce application (astronomy shop) that demonstrates distributed tracing, metrics, and logging.

**What you'll accomplish:**
- [ ] Create an EKS cluster optimized for OTel Demo
- [ ] Configure VPC CNI with prefix delegation to support 110 pods/node
- [ ] Deploy OTel Demo with resource-optimized configuration
- [ ] Verify all services are running
- [ ] Access the Astronomy Shop frontend
- [ ] Understand the deployed services and architecture

## Prerequisites

**Required Tools:**
- AWS CLI v2 configured with credentials
- kubectl v1.27+
- helm v3.12+
- eksctl v0.175+
- jq (for JSON parsing)

**AWS Requirements:**
- AWS account 
- ~$75/month for 5 × t3.small nodes (covered by free tier credits for ~2.5 months)

**Knowledge Requirements:**
- Basic Kubernetes concepts (pods, services, deployments)
- Understanding of AWS EKS
- Familiarity with Helm charts

## Version Information

- Kubernetes / EKS: 1.33
- Instance Type: 5 × t3.small (2GB RAM each)

## Directory Structure

```
00-otel-demo-app/
├── README                              # This file
└── src/
    ├── eks-cluster-config.template.yaml  # Cluster config template (placeholders)
    ├── eks-cluster-config.yaml           # Generated cluster config (after running install script)
    ├── eks-cluster.env                   # Environment variables (created by you, gitignored)
    ├── eks-cluster.env.example           # Example env file - copy this to eks-cluster.env
    ├── install-eks-cluster.sh            # EKS cluster creation script
    ├── install-otel-demo-app.sh          # OTel Demo installation script
    ├── cleanup-otel-demo.sh              # Cleanup script (removes OTel Demo)
    └── otel-demo-app-values.yaml         # Helm values - OTel Demo configuration
```

**File Roles:**
- `eks-cluster.env.example` → Copy to `eks-cluster.env` and fill in your ARN
- `eks-cluster-config.template.yaml` → Template with placeholders, don't edit directly
- `eks-cluster-config.yaml` → Auto-generated by `install-eks-cluster.sh` from template + env
- `otel-demo-app-values.yaml` → Helm values controlling which services run and resource limits

## OpenTelemetry Demo Application

### What Is It?

The OTel Demo simulates a **real-world e-commerce application** with:
- **Multi-language microservices** (Go, .NET, Python, TypeScript, C++, Java, Rust, Ruby, PHP)
- **Realistic shopping features** (browse products, cart, checkout, payment)
- **Built-in observability** (distributed tracing, metrics, logs)
- **Load generator** for realistic traffic patterns

**Application Name:** "Astronomy Shop"
- Browse astronomy products (telescopes, star charts, etc.)
- Add items to cart
- Complete checkout process
- View order confirmation

### Architecture

```
User → frontend (TypeScript) → frontend-proxy (Envoy)
                ↓
    ┌───────────┴────────────┐
    ↓                        ↓
  cart                 product-catalog
(.NET + Valkey)       (Go + PostgreSQL)
    ↓                        ↓
  checkout ───────→ payment, currency,
  (Go)               email, shipping, quote
```

**Observability Stack (this deployment):**
```
Services → OTel Collector Agent (per node)
                       ↓
          ┌────────────┴──────────┐
          ↓                       ↓
      Jaeger                 Prometheus
  (in-memory traces)        (metrics)
                                  ↓
                              Grafana
                            (dashboards)
```

> **Note:** OpenSearch is disabled in this deployment due to OOM issues on t3.small nodes.
> Jaeger uses in-memory trace storage. APM Dashboard log panels won't work, but all other
> dashboards (traces, metrics) function normally. Full observability stack can be enabled
> later with larger infra or fewer app services.

### Enabled Services

**Core Application (9 services):**
1. **frontend** - Web UI (TypeScript) - Entry point
2. **frontend-proxy** - Envoy reverse proxy - Routes to backend services
3. **cart** - Shopping cart (.NET + Valkey)
4. **checkout** - Order processing (Go) - Kafka dependency removed
5. **payment** - Payment processing (JavaScript)
6. **currency** - Currency conversion (C++)
7. **recommendation** - Product recommendations (Python)
8. **product-catalog** - Product listings (Go + PostgreSQL)
9. **image-provider** - Product images (nginx)

**Checkout Supporting Services (3 services):**
10. **email** - Order confirmation emails (Ruby)
11. **shipping** - Shipping cost calculation (Rust)
12. **quote** - Quote generation (PHP)

**Infrastructure (3 components):**
13. **valkey-cart** - Redis-compatible cache for cart data
14. **postgresql** - Database for product-catalog
15. **load-generator** - Traffic simulation (Locust/Python)

**Observability (3 components):**
16. **jaeger** - Distributed tracing (in-memory storage)
17. **grafana** - Metrics dashboards (8 pre-configured dashboards)
18. **prometheus** - Metrics collection and storage
19. **otel-collector-agent** - Telemetry collection (DaemonSet, 1 per node = 5 pods)

**Total: ~23+ pods across 5 nodes**

**Disabled Services:**
- ❌ **opensearch** - OOM on t3.small (saves ~526Mi)
- ❌ **kafka** - Not needed after checkout workaround
- ❌ **product-reviews** - Optional UI feature only (saves 81Mi)
- ❌ **llm** - Only used by product-reviews (saves 69Mi)
- ❌ **ad** - Not needed for Ingress learning
- ❌ **accounting** - Requires kafka, not user-facing
- ❌ **fraud-detection** - Requires kafka, not needed
- ❌ **flagd** - Feature flags, optional

### Resource Consumption (Real-World Data)

Based on actual deployment on 5 × t3.small nodes (2GB memory each), **after disabling opensearch:**

**Heavy Components:**
- grafana: ~408Mi
- prometheus: ~359Mi
- load-generator: ~295Mi

**Medium Components:**
- payment: ~120Mi
- otel-collector-agent: 65-110Mi per node (5 total)
- frontend: ~99Mi

**Light Components (<70Mi each):**
- cart, checkout, currency, email, image-provider, product-catalog, quote, recommendation, shipping, valkey-cart, postgresql

**Node Memory Usage (before opensearch disabled):**
```
Node 1: 71%    Node 2: 71%    Node 3: 70%    Node 4: 50%    Node 5: 51%
```
After disabling opensearch, expect further ~10% reduction on the node where it was running.

## Lab Instructions

### Step 1: Create EKS Cluster

**1.1 Setup env file:**

```bash
cd 00-otel-demo-app/src

# Copy example env file
cp eks-cluster.env.example eks-cluster.env

# Edit eks-cluster.env and fill in your IAM user ARN
# Get your ARN:
aws sts get-caller-identity --query Arn --output text
```

**1.2 Run the cluster creation script:**

```bash
chmod +x install-eks-cluster.sh
./install-eks-cluster.sh otel-demo-2 us-east-2
```

This script:
- Reads your ARN from `eks-cluster.env`
- Generates `eks-cluster-config.yaml` from the template
- Runs `eksctl create cluster`

**Expected time:** 15-20 minutes

**1.3 Verify cluster creation:**

```bash
# Verify nodes are ready
kubectl get nodes
# Expected: 5 nodes, all Ready

# Verify prefix delegation is enabled (CRITICAL)
kubectl get daemonset aws-node -n kube-system -o yaml | grep ENABLE_PREFIX_DELEGATION
# Expected: value: "true"

# Verify pod capacity
kubectl get nodes -o custom-columns=NAME:.metadata.name,MAX_PODS:.status.allocatable.pods
# Expected: MAX_PODS = 110 (not 11!)
```

### Step 2: Review Application Configuration

**2.1 Examine the Helm values file:**

```bash
cat otel-demo-app-values.yaml
```

**Key configurations and why they exist:**

| Change | What | Why |
|--------|------|-----|
| opensearch disabled | `opensearch.enabled: false` | OOM kills on t3.small, not needed for Ingress learning |
| checkout patched | `initContainers: []` + `KAFKA_ADDR: ""` | Removes kafka dependency from init container |
| kafka disabled | `kafka.enabled: false` | accounting/fraud-detection disabled, checkout patched |
| load-generator limited | `memory limit: 300Mi` | Default 1500Mi excessive for demo |
| product-reviews disabled | `enabled: false` | Optional UI feature, saves 81Mi |
| llm disabled | `enabled: false` | Only dependency is product-reviews |

### Step 3: Deploy OpenTelemetry Demo

#### Step 3.1: Deploy Manually

**3.1.1 Add Helm repository:**

```bash
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update
```

**3.1.2 Verify chart availability:**

```bash
helm search repo open-telemetry/opentelemetry-demo
```

**3.1.3 Create namespace:**

```bash
kubectl create namespace otel-demo
```

**3.1.4 Install OTel Demo:**

```bash
helm install otel-demo open-telemetry/opentelemetry-demo \
  --namespace otel-demo \
  --values otel-demo-app-values.yaml
```

**Expected output:**
```
NAME: otel-demo
NAMESPACE: otel-demo
STATUS: deployed
REVISION: 1
```

#### Step 3.2: Deploy using Script

```bash
chmod +x install-otel-demo-app.sh
./install-otel-demo-app.sh
```

The script handles: prerequisite checks, Helm repo setup, namespace creation, and Helm install. It prints access instructions on completion.

### Step 4: Validate Deployment

**4.1 Monitor pod startup:**

```bash
kubectl get pods -n otel-demo -w
# Press Ctrl+C when all pods show Running
```

**4.2 Check all pod statuses:**

```bash
kubectl get pods -n otel-demo
```

**Expected (~23 pods):**
```
NAME                               READY   STATUS    RESTARTS   AGE
cart-xxx                           1/1     Running   0          5m
checkout-xxx                       1/1     Running   0          5m
currency-xxx                       1/1     Running   0          5m
email-xxx                          1/1     Running   0          5m
frontend-xxx                       1/1     Running   0          5m
frontend-proxy-xxx                 1/1     Running   0          5m
grafana-xxx                        4/4     Running   0          5m
image-provider-xxx                 1/1     Running   0          5m
jaeger-xxx                         1/1     Running   0          5m
load-generator-xxx                 1/1     Running   2-5        5m
otel-collector-agent-xxx (×5)      1/1     Running   0          5m
payment-xxx                        1/1     Running   0          5m
postgresql-xxx                     1/1     Running   0          5m
product-catalog-xxx                1/1     Running   0-2        5m
prometheus-xxx                     1/1     Running   0          5m
quote-xxx                          1/1     Running   0          5m
recommendation-xxx                 1/1     Running   0          5m
shipping-xxx                       1/1     Running   0          5m
valkey-cart-xxx                    1/1     Running   0          5m
```

**Normal restart behavior:**
- `load-generator`: 2-5 restarts expected (stress-testing the system)
- `product-catalog`: 0-2 restarts (initial database connection)

**4.3 Check resource consumption:**

```bash
kubectl top nodes
kubectl top pods -n otel-demo
```

### Step 5: Access the Application

**5.1 Frontend (Astronomy Shop):**

```bash
kubectl port-forward -n otel-demo svc/frontend-proxy 8080:8080
```
Open: http://localhost:8080

Test: Browse products → Add to cart → Checkout → Order confirmation

**5.2 Jaeger (Distributed Tracing):**

```bash
kubectl port-forward -n otel-demo svc/jaeger 16686:16686
```
Open: http://localhost:16686

**5.3 Grafana (Dashboards):**

```bash
kubectl port-forward -n otel-demo svc/grafana 3000:80
```
Open: http://localhost:3000/grafana/

> **Note:** No login required - anonymous access is enabled.
> Grafana is served under the `/grafana/` sub-path. Opening `http://localhost:3000/` alone will not show dashboards.

**Available Dashboards (8):**
1. Demo Dashboard
2. APM Dashboard (Jaeger, Prometheus, OpenSearch) - log panels inactive without opensearch
3. Spanmetrics Demo Dashboard
4. Cart Service Exemplars
5. OpenTelemetry Collector
6. Linux
7. PostgreSQL
8. [Image-Provider] NGINX Metrics

**Datasources:**
- Prometheus (`webstore-metrics`) - metrics queries ✅
- Jaeger (`webstore-traces`) - trace queries ✅
- OpenSearch (`webstore-logs`) - log queries ❌ (opensearch disabled)

## Common Deployment Issues

### Issue 1: product-catalog CrashLoopBackOff - "failed to allocate IP address"

**Root Cause:** Prefix delegation not enabled. Node only has ~11 pod IPs instead of 110.

**Verify:**
```bash
kubectl get nodes -o custom-columns=NAME:.metadata.name,MAX_PODS:.status.allocatable.pods
# Must show 110, not 11
```

**Fix:**
```bash
kubectl set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true
# Then recycle nodes
```

### Issue 2: checkout Stuck in Init:0/1

**Root Cause:** Init container waiting for kafka.

**Fix:** Ensure `otel-demo-app-values.yaml` has:
```yaml
checkout:
  initContainers: []
  env:
    - name: KAFKA_ADDR
      value: ""
```

### Issue 3: opensearch OOMKilled / CrashLoopBackOff

**Root Cause:** t3.small nodes don't have enough memory for opensearch (~1GB needed).

**Fix:** Disable opensearch (already done in current config):
```yaml
opensearch:
  enabled: false
```

### Issue 4: Nodes Above 90% Memory

**Options (in order):**
1. Scale nodes: `eksctl scale nodegroup --cluster=<name> --name=<name> --nodes=6`
2. Disable product-reviews + llm (saves ~150Mi)
3. Disable opensearch (saves ~526Mi)

### Issue 5: load-generator Restarting

2-5 restarts is normal (it stress-tests the system). If >10 restarts, increase its memory limit in values file.

### Issue 6: Pods Not Redistributing After Scaling Nodes

New nodes don't automatically pull existing pods. Two approaches:

**Restart deployments:**
```bash
kubectl rollout restart deployment -n otel-demo
```

**Drain the overloaded node:**
```bash
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data --force
kubectl uncordon <node-name>
```

> **Note:** StatefulSets don't restart with `rollout restart deployment`.
> Delete the pod directly: `kubectl delete pod <pod-name> -n otel-demo`

## Key Concepts

### Prefix Delegation

Without: t3.small gets ~11 pod IPs per node.
With: t3.small gets ~110 pod IPs per node (via /28 CIDR blocks).
OTel Demo needs ~23 pods. Without prefix delegation, IP exhaustion crashes pods.

### Resource Requests vs Limits

```yaml
resources:
  requests:
    memory: "120Mi"  # Scheduler guarantees this on the node
  limits:
    memory: "300Mi"  # Pod is OOMKilled if it exceeds this
```

## Validation Checklist

Before proceeding to Ingress demos, verify:

- [ ] ~23 pods Running (load-generator may have a few restarts)
- [ ] No pods in CrashLoopBackOff or Init state
- [ ] Node pod capacity shows 110 (not 11)
- [ ] Nodes at <85% memory
- [ ] Frontend accessible at http://localhost:8080 (via port-forward)
- [ ] Jaeger accessible at http://localhost:16686
- [ ] Grafana accessible at http://localhost:3000/grafana/
- [ ] Shopping flow works: browse → cart → checkout → confirmation

## Cleanup

**Remove OTel Demo only:**
```bash
cd 00-otel-demo-app/src
./cleanup-otel-demo.sh
```

**Delete EKS cluster (removes everything):**
```bash
eksctl delete cluster --name <cluster-name>
(OR)
eksctl delete cluster -f eks-cluster-config.yaml
```

## Next Steps

- **Demo 1.1:** Install AWS Load Balancer Controller
- **Demo 1.2:** Install Traefik Ingress Controller

